<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2020-06-10T01:02:08-04:00</updated><id>/feed.xml</id><title type="html">Mohan Zalake</title><subtitle></subtitle><author><name>Mohan Zalake</name></author><entry><title type="html">CPR Voice Assistant</title><link href="/virtual%20agents/2020/06/08/cpr-voice-assistant/" rel="alternate" type="text/html" title="CPR Voice Assistant" /><published>2020-06-08T22:36:29-04:00</published><updated>2020-06-08T22:36:29-04:00</updated><id>/virtual%20agents/2020/06/08/cpr-voice-assistant</id><content type="html" xml:base="/virtual%20agents/2020/06/08/cpr-voice-assistant/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Sudden Cardiac arrest is a leading cause of death in the United States [6]. The majority of cardiac arrests occur out of the hospital (OHCA) with less than an 8% survival rate [1]. Even though the majority of cardiac arrests are OHCA, 70% of Americans feel helpless in an emergency situation because they are unsure how to perform CPR [2]. However, there is a lack of existing tools and instructions on how to perform CPR during an emergency situation. A recent study showed that half of the nation’s emergency dispatchers do not have public safety answering points for performing CPR, and very few provide Hands-Only instructions [7]. Also, when asking Siri to provide steps on how to perform CPR she searches the Web which may cause delays in an emergency situation. Therefore, we built a Hands-Only Cardiopulmonary resuscitation (CPR) Assistant which is a system designed to help users go through the Hands-Only CPR process with ease while performing the procedure. The American Heart Association promotes Hands-Only CPR in an emergency OHCA situation [2], since it has been shown to be as effective as traditional CPR with rescue breaths, and bystanders are more likely to step in when not performing mouth-to-mouth. Our system provides clear speech instructions to perform Hands-Only CPR in an emergency OHCA situation through a dialogue based system. The CPR Assistant is a .NET Web based application written in C#, javascript and HTML. The system takes speech from the user as input and presents information using synthesized speech, as well as provides audio and visuals to aid in performing CPR. We tested the CPR Assistant through iterative user testing, and tested the final system on a total of 23 participants (M = 24.4, SD = 3.9, 4 females). Overall, the participants were satisfied with our system (M = 3.7, SD = 0.55) based on a five point Likert scale.&lt;/p&gt;

&lt;h2 id=&quot;hands-only-cpr-assistant&quot;&gt;Hands-only CPR Assistant&lt;/h2&gt;

&lt;p&gt;Hands-Only Cardiopulmonary Resuscitation (CPR) Assistant is a system designed to help users go through the Hands-Only CPR process with ease while performing the procedure. It is a .NET Web based application written in C#, javascript and HTML. The CPR Assistant assesses the situation and provides instruction through dialogue from the user. When opened, the system immediately addresses the user by stating “Hello I am Mike. I can help you perform CPR. Is this an emergency?”. Then based on the user’s response the CPR Assistant will either provide simple steps on how to perform CPR, if it is not an emergency, or assess the situation and walk the user through performing CPR, if it is an emergency. Our CPR Assistant provides instructions on how to perform CPR based off of the steps from the American Red Cross and the American Heart Association [2,4]. The system will walk the user through: checking if in a safe location, checking if the person is breathing, determining if CPR should be performed, calling 911, and providing compressions.&lt;/p&gt;

&lt;p&gt;Our system provides visual feedback as well as audio feedback. The system tracks the conversation and prints out the conversation in real time, allowing the user to see their statements as well as the systems. This provides a way for the user to track the conversation, as well as be able to read the instructions again if in a noisy or busy environment. It also allows the user to see if the system misunderstood them, which if misunderstood the system could provide wrong instructions when performing CPR. The system also provides images in real time based on the current step and situation to instruct the user to perform CPR. For example, the system will provide an image to illustrate how to check if breathing, when to call 911, and how to position your hands on the chest of the person who needs CPR. Our system also provides audio feedback by providing a beat to follow along to, to do 100 compressions per minute in an emergency situation. This makes CPR compressions easier to perform since the user would not have to count and just follow along to the beat. Through providing images and sound as well as audio instructions, we were able to reach our stretch goals and develop a well-rounded system that provides feedback to the user.&lt;/p&gt;

&lt;h2 id=&quot;pilot-testing&quot;&gt;Pilot Testing&lt;/h2&gt;

&lt;p&gt;While developing the system we iteratively tested the system with different users to assess the speech recognition, speech synthesis, and functionality of the system&lt;/p&gt;

&lt;p&gt;We first tested the speech recognition by having 5 participants (M = 24 yrs, 3 female) state six sentences that were preset. The sentences were selected from a recording of a CPR 911 dispatch call [14]. For example, “He is not breathing” and “I don’t know how to perform CPR”. We then calculated the Word Error Rate (WER) which was 22%, 51% for non-native English speakers (N = 2) and 2.8% for native English speakers (N = 3). To test the speech synthesis, we played four different sentences (e.g., Are you in a safe location?) at different speeds and with a male and female voice. We then asked the participants what voice and what speed they preferred. The majority consensus was the male voice played at -5% speed.&lt;/p&gt;

&lt;p&gt;After developing the initial prototype, we then tested the functionality with 10 participants (M = 23 yrs, 6 female). We allowed the participants to freely interact with the system and did not constrain their input. This allowed us to test for any bugs, implement any functionality that we missed, and gain more examples of dialogue to train and test the system on for recognition. For example, we added was more support for “I don’t know” statements, if the user moved to a safe location, and if help arrived.&lt;/p&gt;

&lt;h2 id=&quot;user-testing&quot;&gt;User Testing&lt;/h2&gt;

&lt;p&gt;The final user testing was accomplished through a round robin method, where different people walked around and interacted with systems. It was done in a small sub-area of a classroom and therefore was a noisy and busy environment.&lt;/p&gt;

&lt;p&gt;We had a total of 24 participants interact with our system, however one user missed questions on the post survey which was used for analysis so we only examined the responses for 23 participants (M = 24.4, SD = 3.9, 4 females). Ten of the participants were non-native English speakers, and only nine of the participants knew CPR before using the system.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;In order to compute the Word Error Rate (WER) the recordings were transcribed in order to have the user’s intended response. It was then compared to what the system recognized, which was located in the logs. In total we had an 8.9% WER, 10.3% for non-native English speakers (N = 10) and 8% for native English speakers (N = 13). Overall, this is an improvement from the 22% WER from the pilot testing. Also, since the testing occurred in a noisy and busy environment the WER would probably dramatically improve in a quieter environment.&lt;/p&gt;

&lt;p&gt;For the 11 Likert questions, the responses were converted to a number based system (e.g., Strongly agree = 5 and Strongly disagree = 1). The scores from the negative statements such as “I thought there was too much inconsistency in this system” and “I found the system cumbersome to use” were then inverted. The average score for each participant was then calculated, and then the total average was 3.7 (SD = 0.55) which illustrates that participants were generally satisfied with our system.&lt;/p&gt;

&lt;p&gt;Multiple regression analysis was performed to determine how individual factors affected average user satisfaction. For our data analysis, we used continuous scale variable called average user satisfaction (scale between 1 to 5) as dependent variable. It was calculated as an average of individual ratings from usability scale. We also used seven variables as independent variables, namely – English as first language (Yes/No), previous knowledge of CPR (Yes/No), WER, duration of conversation (in seconds), number of clarification request from system to user (e.g., system does not know), number of user and system dialogue turns, and task success (Yes/No). Task success implies if system was able to either assist user to perform CPR in emergency case or help them learn. Based on above data, multiple regression was performed to calculate coefficients and average user satisfaction using SPSS tool. Although no significant results were obtained from our analysis , results indicate that three variables with negative coefficients (English as first language, WER and number of clarification request from system to user) reduced the user satisfaction. Considering the weights of each coefficient, WER and user’s first language were two major factors in lower user satisfaction. These results suggest that improving the ASR can possibly increase the average user satisfaction. There can be several possible reasons like noisy environment, user’s English accent etc. for word errors during speech recognition. Reducing the background noise or training ASR with different English accents can improve the system. Another factor which negatively affected user satisfaction was number of clarification request from system to user. Considering the scope of this project, system was trained for very limited number of responses. Training the system with large number of possible inputs along with low WER can possibly reduce the number of clarification requests from system. Although system was implemented with multiple fallback responses for clarification requests, improving the system with extensive implemented just in time instructions from system can help reduce number of clarifications.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We created a Hands-Only Cardiopulmonary Resuscitation (CPR) Assistant, which is a system designed to help users go through the Hands-Only CPR process with ease while performing the procedure. It is a .NET Web based application and it assesses the situation and provides instructions through dialogue from the user. Overall, the participants were satisfied with our system and thought it was easy to use. This system demonstrates the applicability of a spoken dialogue system in helping individuals in emergency scenarios. This proof of concept can be extended to a real world application by improving it further and making a hands-free CPR assistant&lt;/p&gt;</content><author><name>Mohan Zalake</name></author><category term="Virtual Agents" /><summary type="html">Introduction</summary></entry><entry><title type="html">CPR Training in Virtual Reality</title><link href="/virtual%20reality/2020/06/08/cpr-training-in-virtual-reality/" rel="alternate" type="text/html" title="CPR Training in Virtual Reality" /><published>2020-06-08T20:55:37-04:00</published><updated>2020-06-08T20:55:37-04:00</updated><id>/virtual%20reality/2020/06/08/cpr-training-in-virtual-reality</id><content type="html" xml:base="/virtual%20reality/2020/06/08/cpr-training-in-virtual-reality/">&lt;hr /&gt;

&lt;p&gt;layout: post author: Mohan Zalake
title: CPR Training in Virtual Reality
date: 2020-06-09T00:38:51.270Z
thumbnail: /assets/img/posts/vrtraining.jpg
category: jekyll
summary: VR based trainer to perform CPR. Built in a day. Hackathon Winner -
  Best First-Time Hack.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;inspiration&quot;&gt;Inspiration&lt;/h2&gt;

&lt;p&gt;Cardiac arrest – an electrical malfunction in the heart that causes an irregular heartbeat (arrhythmia) and disrupts the flow of blood to the brain, lungs and other organs – is a leading cause of death. Each year, more than 350,000 out-of-hospital cardiac arrests occur in the United States.When a person has a cardiac arrest survival depends on immediately getting CPR from someone nearby.Almost 90 percent of people who suffer out-of-hospital cardiac arrests die.CPR especially if performed in a right way, can double or triple a person’s chance of survival. There is a strong need of making people aware and trained to perform CPR. Most of the deaths occur due to having less information about CPR or by people performing it in a wrong way or slow way.&lt;/p&gt;

&lt;h2 id=&quot;what-it-does&quot;&gt;What it does&lt;/h2&gt;

&lt;p&gt;CPR.VR is an application which first teaches user how to perform CPR and what are the critical measures one should take while performing CPR. This application also depicts the actual hand compression gesture one should perform while giving CPR.The application also gives you feedback on the CPR therapy given by you to virtual patients by counting the right gestures of compression made by you which is required in the real CPR therapy at the appropriate rate.&lt;/p&gt;

&lt;h2 id=&quot;how-we-built-it&quot;&gt;How we built it&lt;/h2&gt;

&lt;p&gt;The application is developed and designed using C# in unity engine and works on Oculus-Rift VR Headset with leap motion sensor to track hand movements.We have used leap motion hand tracking to track the hand and its position and the right gesture and counted the number of valid gestures made by the user in an appropriate time at an appropriate position and returned the feedback accordingly We have developed this application following all the rules of Human computer Interaction and used Shneiderman’s “Eight Golden Rules to design the interface for the application.&lt;/p&gt;

&lt;h2 id=&quot;challenges-we-ran-into&quot;&gt;Challenges we ran into&lt;/h2&gt;

&lt;p&gt;We were new to hand gesture recognition technology. First challenge we faced was to choose which hand gesture technology to use. Then to figure out the different hand gestures recognition hardware’s API and how to integrate them with Unity. We also faced difficulty in tracking hand gesture with right calibration on our Game Object. We first tried the Myo armband and was unable to perform the compression gesture with right calibration on unity.Then we switched to leap Motion and made its setup from the scratch. Learned about its API function calls. We started our project on GearVR because of switching the hand gesture recognition device.We were forced to switch the VR device too as Leap motion was not working with GearVR.&lt;/p&gt;

&lt;h2 id=&quot;accomplishments-that-were-proud-of&quot;&gt;Accomplishments that we’re proud of&lt;/h2&gt;

&lt;p&gt;1st Hackathon experience for 3 of our Teammates.Learning New Technologies-Leap Motion and successfully implementing the application on Oculus-Rift VR. Fixing all the bugs and making application work as planned.&lt;/p&gt;

&lt;h2 id=&quot;what-we-learned&quot;&gt;What we learned&lt;/h2&gt;

&lt;p&gt;Unity Development using Leap Motion. Development of Game on Oculus VR. Animating the object in Unity&lt;/p&gt;

&lt;h2 id=&quot;whats-next-for-cprvr&quot;&gt;What’s next for CPR.VR&lt;/h2&gt;

&lt;p&gt;An Upgraded version where a user can perform the artificial ventilation and mouth to mouth ventilation at the right time after giving CPR&lt;/p&gt;

&lt;h2 id=&quot;built-with&quot;&gt;Built With&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Adobe Fuse&lt;/li&gt;
  &lt;li&gt;C#&lt;/li&gt;
  &lt;li&gt;Leap Motion&lt;/li&gt;
  &lt;li&gt;Mixamo&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://devpost.com/software/built-with/oculus-gear-vr&quot;&gt;&lt;/a&gt;Oculus Gear VR&lt;/li&gt;
  &lt;li&gt;Unity&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;try-it-out&quot;&gt;Try it out&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sayakbiswas/hackriddle-2017&quot; title=&quot;https\://github.com/sayakbiswas/hackriddle-2017&quot;&gt; GitHub Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Mohan Zalake</name></author><category term="Virtual Reality" /><summary type="html"></summary></entry><entry><title type="html">Virtual Reality Shopping Experience</title><link href="/2020/06/08/virtual-reality-shopping-experience/" rel="alternate" type="text/html" title="Virtual Reality Shopping Experience" /><published>2020-06-08T00:00:00-04:00</published><updated>2020-06-08T00:00:00-04:00</updated><id>/2020/06/08/virtual-reality-shopping-experience</id><content type="html" xml:base="/2020/06/08/virtual-reality-shopping-experience/">&lt;p&gt;&lt;strong&gt;Executive Summary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Virtual reality (VR) has become more and more popular among people. So how can merchants use this emerging technology to attract more people to go to the actual store and make their purchase there? After conducting user research, the design team found that many people prefer shopping to be fun and social experience. So the design ideas were focused more on fun and social aspect of the experience. We also found that shopping experience also differs by type of product. To narrow our focus, we focused on building experience for shopping clothes. Initial design of system largely based on using VR as a platform to solve breakdowns and attract users to physical store. After the mid-evaluation feedback, we found that other making experience fun we should also concentrate on making experience smooth. Thus, our previous design was re-iterated to make transition as smooth as possible. This involves avoiding typing in VR, irregular hand movements etc. After conducting the think-aloud user test, design team found that users prefer interactions options on top than on bottom of 3D screen. Also we noticed that users were strained when they were asked to move their heads again and again. Using these observations and feedback, user interface was redesigned to fit in VR friendly field of view and final design is demonstrated as an interactive prototype.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With the development of internet, an increasing number of people are shopping online. Thus physical stores are facing great crisis than never before. VR is thought to be a technology that will change people’s life greatly. To use this newly created technology to attract more people to go out of their room, visit the physical stores and increase the sales volume of shops has become a big and challenging problem. Team was focused to design a shopping journey in VR such that it can enhance the possibility of visiting and successfully purchasing in the physical store. The design team first conducted a user research to understand what motivates users to go to physical store and to identify the shortcomings and advantages provided by the real store compared with other shopping options. Based on the data from the focus group, user personas were created, which gave the design team a solid base for brainstorming the design ideas. Based on these ideas, the scenarios and storyboards were created to address the breakdown and opportunities. Task flows and wireframes were designed to best represent the shopping journey using VR. While getting feedback from the client, an improved version was presented after numerous iteration and user tests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Context / Focus Setting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our goal is to design a shopping experience that starts users’ purchase journey in VR and motivate them to visit real stores at the end of journey. Also, address the user needs along with merchant needs by identifying the shortcomings and advantages in experience provided by retail stores when compared to other shopping options.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discovery: User Research&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One focus group was conducted with graduate students at UF. A total of 7 students participated with 4 females and 3 males. The focus group was conducted about an hour long. Among these 7 students, 2 explicitly said that they would not use VR to shopping since there is no difference between VR shopping and shop online.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Analysis: Walk the Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After conducting the user research, the responses the researchers gave have been transcribed into post-its. Similar responses are organized and then the affinity diagram was used to analyze the results. 6 user needs were summarized based on priorities (high to low) from the affinity diagram and are listed below:&lt;/p&gt;

&lt;p&gt;1) I want to shopping for fun/social experience.&lt;/p&gt;

&lt;p&gt;2) I like to find/buy things easily.&lt;/p&gt;

&lt;p&gt;3) I need proper details of product.&lt;/p&gt;

&lt;p&gt;4) I need haptic feedback/measurement.&lt;/p&gt;

&lt;p&gt;5) I need personalized experience;&lt;/p&gt;

&lt;p&gt;6) I don’t want to waste a lot of time in shopping.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Brainstorm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Based on the personas the design team created, brainstorming was conducted to come up with design ideas. Since the final purpose of this VR project is to attract user to go to the actual store, most of our ideas focus on this and the top 6 user needs, especially the first one - shop for social experience. These ideas were ranked according to the frequency mentioned in the focus group and importance, from 1-3 (the highest is 3). By combining these ideas, the final design focus more on the following aspects: 1) can socialize and interact with friends 2) can have fun in VR by exploring the virtual world and earn points there. These earned points can be spent while buying clothes or to unlock VR premium content. 3) can try all the clothes in VR on his/her own avatar 4) can design clothes and share it. Here user can even earn money if his/her design idea is popular among people. In the next section, we will show how the design ideas were combined and what functions this VR has by scenarios and storyboards.&lt;/p&gt;

&lt;p&gt;Based on the personas, user needs and design ideas, scenarios were created to illustrate how personas would use our system and solve their problems. Each scenario covers a different sets of design concept.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The design team was intended to design a better shopping experience with VR such that the shopper can begin their purchase journey in a virtual store and finish their customer journey in a physical store. To accomplish this task, the group conducted the focus group to do a research and determine what the user needs in their shopping experience. Affinity diagram was used to find the top 6 user needs. 3 different user personas were created with each one particularly emphasizing on 2 user needs. Various design ideas came out through brainstorming and the scenarios and corresponding storyboards help build a bridge between the user needs and the design ideas. Wireframes of the design were created to illustrate the actual flow of shopping experience. After the midterm, feedback was received to improve the wireframes and make the scenarios highlighted on the wireframe. User tests were conducted to evaluate the prototype that is build based on the wireframe. After iteration, final prototype was finished and the corresponding video sketch was created to show how this application is used to improve the shopping experience and encourage people to purchase at the real store.&lt;/p&gt;</content><author><name>Mohan Zalake</name></author><summary type="html">Executive Summary</summary></entry><entry><title type="html">Welcome to devlopr !</title><link href="/jekyll/2019/05/22/welcome-to-jekyll/" rel="alternate" type="text/html" title="Welcome to devlopr !" /><published>2019-05-22T05:05:23-04:00</published><updated>2019-05-22T05:05:23-04:00</updated><id>/jekyll/2019/05/22/welcome-to-jekyll</id><content type="html" xml:base="/jekyll/2019/05/22/welcome-to-jekyll/">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name>John Doe</name></author><category term="jekyll" /><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry></feed>